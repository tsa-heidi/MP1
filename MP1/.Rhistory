training <- read_csv("data/train.csv")
test <- read_csv("data/test.csv")
sample_submission <- read_csv("data/sample_submission.csv")
training <- read_csv("data/train.csv")
test <- read_csv("data/test.csv")
sample_submission <- read_csv("data/sample_submission.csv")
clear
# Chunk 1: setup
# Load all your packages here:
library(tidyverse)
library(scales)
# Set default behavior for all code chunks here:
knitr::opts_chunk$set(
echo = TRUE, warning = FALSE, message = FALSE,
fig.width = 16/2, fig.height = 9/2
)
# Set seed value of random number generator here. This is in order to get
# "replicable" randomness, so that any results based on random sampling or
# resampling are replicable everytime you knit this file. Why use a seed value
# of 76? For no other reason than:
# https://www.youtube.com/watch?v=xjJ7FheCkCU
set.seed(76)
# Chunk 2
training <- read_csv("data/train.csv")
test <- read_csv("data/test.csv")
sample_submission <- read_csv("data/sample_submission.csv")
# Chunk 4
glimpse(sample_submission)
# Chunk 5
outcome_var_plot <- ggplot(training, aes(x = SalePrice))+
geom_histogram()
# Chunk 6
predic_var_plot <- ggplot(training, aes(x = LotArea)) +
geom_histogram()
# Chunk 7
training_plot <- ggplot(training, aes(x = LotArea, y = SalePrice)) +
geom_point()
training_plot
# Chunk 8
#Randomly shuffle the data
set.seed(21)
training<-training[sample(nrow(training)),]
#Create 10 equally size folds
folds <- cut(seq(1,nrow(training)),breaks=10,labels=FALSE)
# list of df we want to try
df_lst <- seq(2,35, by = 1)
#create a list of avg_rmsle_hat values found from differnt df
avg_rmsle_hat_lst <- c()
for (d in df_lst){
#keep a list of RMSLE_hat for this fi
RMSLE_hat_list <- c()
#Perform 10 fold cross validation
for(i in 1:10){
#Segement your data by fold using the which() function
testIndexes <- which(folds==i,arr.ind=TRUE)
testData <- training[testIndexes, ]
trainData <- training[-testIndexes, ]
#Use the test and train data partitions to cross validate
fitted_spline_model <- smooth.spline(x = trainData$LotArea, y = trainData$SalePrice, df = d)
# Extract data frame of info based on fitted model:
fitted_spline_model_points <- fitted_spline_model %>%
broom::augment()
fitted_spline_model_points
predicted_points <- predict(fitted_spline_model, x = testData$LotArea) %>%
as_tibble()
testData <- testData %>%
mutate(SalePrice_hat = predicted_points$y)
RMSLE_hat <- (MLmetrics::RMSLE(y_pred = testData$SalePrice_hat, y_true = testData$SalePrice))
#add the new RMSLE_hat to the list of RMSLE_hat values
RMSLE_hat_list <- c(RMSLE_hat_list, RMSLE_hat)
}
#find the average RMSE and add to avg_rmsle_hat_lst
avg_rmsle_hat_lst <- c(avg_rmsle_hat_lst, mean(RMSLE_hat_list))
}
# Chunk 9
#make a dataframe from list of df and rmsle values
dataf <- data.frame(df = df_lst, RMSLE = avg_rmsle_hat_lst)
# Chunk 10
optimal_df_plot <- ggplot(dataf, aes(x = df ,y = RMSLE)) +
geom_point()
optimal_df_plot
optimal_df <- which.min(avg_rmsle_hat_lst)+1
message(optimal_df)
# Chunk 11
df_star <- 22
# Chunk 12
fitted_spline_model <- smooth.spline(x = training$LotArea, y = training$SalePrice, df = 22)
# Extract data frame of info based on fitted model:
fitted_spline_model_points <- fitted_spline_model %>%
broom::augment()
fitted_spline_model_points
# Plot fitted model on training data:
training_plot +
geom_line(data = fitted_spline_model_points, aes(x = x, y = .fitted), col = "blue", size = 1)
# Chunk 13
predicted_points <- predict(fitted_spline_model, x = test$LotArea) %>%
as_tibble()
predicted_points
# Plot!
ggplot() +
geom_point(data = predicted_points, aes(x = x, y = y))
# Chunk 14
submission <- test %>%
mutate(SalePrice_hat = predicted_points$y) %>%
select(LotArea,SalePrice_hat)
write_csv(submission, path = "data/submission.csv")
View(dataf)
